<!DOCTYPE html>
<html lang="en"
  dir="ltr">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width">



<link rel="icon" type="image/ico" href="//localhost:1313//favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="//localhost:1313//favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="//localhost:1313//favicon-32x32.png">
<link rel="icon" type="image/png" sizes="192x192" href="//localhost:1313//android-chrome-192x192.png">
<link rel="apple-touch-icon" sizes="180x180" href="//localhost:1313//apple-touch-icon.png">

<meta name="description" content="Exploring the reinforcement learning process and its application in complex problems, including a complete example with Python code"/>



<title>
    
    Reinforcement Learning Process and Its Application in Complex Problems | Sajjad Momeni
    
</title>

<link rel="canonical" href="//localhost:1313/posts/research/lunar-lander/lunar-lander/"/>

<meta property="og:url" content="//localhost:1313/posts/research/lunar-lander/lunar-lander/">
  <meta property="og:site_name" content="Sajjad Momeni">
  <meta property="og:title" content="Reinforcement Learning Process and Its Application in Complex Problems">
  <meta property="og:description" content="Exploring the reinforcement learning process and its application in complex problems, including a complete example with Python code">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-03-13T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-03-13T00:00:00+00:00">
    <meta property="article:tag" content="Machine-Learning">
    <meta property="article:tag" content="Reinforcement-Learning">
    <meta property="article:tag" content="Deep-Learning">
    <meta property="article:tag" content="Python">








<link rel="stylesheet" href="/assets/combined.min.829116ec33ddb3bb868c9f590a489ccacb598fe5b75d430f94dbaf77d45c338d.css" media="all">





</head>





<body class="auto">

  <div class="content">
    <header>
      

<div class="header">

    

    <h1 class="header-title">
        
    </h1>

    <div class="flex">
        

        
    </div>

    

</div>

    </header>

    <main class="main">
      
<div class="single-container">
  <header>
    

<nav class="main-nav">
  
    
    <a href="/" class="nav-link">/home</a>
    <a href="/posts/" class="nav-link">/posts</a>
    <a href="/about/" class="nav-link">/about</a>
  
</nav>

<style>
.main-nav {
  margin-bottom: 4rem;
  display: flex;
  gap: 0.5rem;
  flex-wrap: wrap;
  font-family: monospace;
}

.nav-link {
  color: inherit;
  text-decoration: none;
  font-size: 1rem;
  font-family: monospace;
}

.nav-link:hover {
  text-decoration: underline;
}

 
.rtl .main-nav a {
  margin-right: 0;
  margin-left: 1.5rem;
}
</style> 
  </header>

  <main>
    <article class="content">
      <h1 class="content-title">Reinforcement Learning Process and Its Application in Complex Problems</h1>
      <p>











<figure class="">

    <div>
        <img loading="lazy" alt="Lunar Lander Training Demo" src="hero.gif" >
    </div>

    
</figure>
</p>
<p>Reinforcement Learning (RL) is a significant branch of machine learning where an agent interacts with an environment and improves its behavior based on the rewards it receives. In this article, we will explore the reinforcement learning process, answer some of your questions, and provide a complete example with Python code, including explanations of arrays, matrices, and the model training process.</p>
<hr>
<h2 class="heading" id="1-introduction-to-reinforcement-learning">
  1. Introduction to Reinforcement Learning
  <a class="anchor" href="#1-introduction-to-reinforcement-learning">#</a>
</h2>
<p>In reinforcement learning, an agent is placed in an environment and takes various actions to change its state. After each action, the agent receives a reward. The ultimate goal is to learn an optimal policy that selects actions in each state to maximize long-term rewards.</p>
<h3 class="heading" id="key-components-of-reinforcement-learning">
  Key Components of Reinforcement Learning:
  <a class="anchor" href="#key-components-of-reinforcement-learning">#</a>
</h3>
<ul>
<li><strong>Agent:</strong> The entity that takes actions and learns.</li>
<li><strong>Environment:</strong> The world in which the agent operates.</li>
<li><strong>State:</strong> Represents the current situation of the environment.</li>
<li><strong>Action:</strong> The actions the agent can take.</li>
<li><strong>Reward:</strong> A signal indicating the agent&rsquo;s performance in the environment.</li>
<li><strong>Policy:</strong> A method for selecting actions in each state.</li>
</ul>
<hr>
<h2 class="heading" id="2-q-learning-algorithm">
  2. Q-Learning Algorithm
  <a class="anchor" href="#2-q-learning-algorithm">#</a>
</h2>
<p>The core Q-update formula is as follows:</p>
<p>$$
Q(s, a) \leftarrow (1 - \alpha)Q(s, a) + \alpha\left(r + \gamma \max_{a&rsquo;} Q(s&rsquo;, a&rsquo;)\right)
$$</p>
<p>Where:</p>
<ul>
<li>$\alpha$: Learning rate</li>
<li>$\gamma$: Discount factor</li>
<li>$r$: Received reward</li>
<li>$s&rsquo;$: Next state</li>
</ul>
<hr>
<h2 class="heading" id="3-challenges-of-continuous-state-spaces-and-the-need-for-discretization">
  3. Challenges of Continuous State Spaces and the Need for Discretization
  <a class="anchor" href="#3-challenges-of-continuous-state-spaces-and-the-need-for-discretization">#</a>
</h2>
<p>In many real-world environments, state variables are continuous, such as speed, position, angle, etc., which can take any value within a specific range. For example, the speed of an object might be 10.0, 10.2, or 9.8 km/h. In such cases, creating a Q-table for every possible value (infinite states) is impossible.</p>
<h3 class="heading" id="why-use-discretization">
  Why Use Discretization?
  <a class="anchor" href="#why-use-discretization">#</a>
</h3>
<ul>
<li>
<p><strong>Covering Infinite States:</strong><br>
If each state variable is continuous, the number of possible states becomes infinite. Discretization reduces the number of possible states by dividing continuous ranges into finite bins.</p>
</li>
<li>
<p><strong>Easy Indexing:</strong><br>
By converting continuous values into discrete indices, it becomes easier to access values in the Q-table.</p>
</li>
<li>
<p><strong>Improved Learning:</strong><br>
Even if there are minor differences (e.g., 10.0 vs. 10.2), the agent can use similar experiences within the same bin.</p>
</li>
</ul>
<h3 class="heading" id="simple-example">
  Simple Example:
  <a class="anchor" href="#simple-example">#</a>
</h3>
<p>Suppose speed ranges from 0 to 10 km/h, and we divide this range into 10 bins:</p>
<ul>
<li>Bin 0: Speed 0 to 1</li>
<li>Bin 1: Speed 1 to 2</li>
<li>â€¦</li>
<li>Bin 9: Speed 9 to 10</li>
</ul>
<p>Thus, each continuous value is assigned to one of these 10 bins, and the bin is used as an index in the Q-table.</p>
<hr>
<h2 class="heading" id="4-practical-example-training-an-agent-in-the-lunarlander-environment">
  4. Practical Example: Training an Agent in the LunarLander Environment
  <a class="anchor" href="#4-practical-example-training-an-agent-in-the-lunarlander-environment">#</a>
</h2>
<p>In this section, we provide a complete example of creating and training a Q-Learning-based agent for the <strong>LunarLander-v3</strong> environment from the <strong>Gymnasium</strong> library. In this example, 8 state variables are divided into 10 bins, and a Q-table with dimensions<br>
[(10, 10, 10, 10, 10, 10, 10, 10, 4)]<br>
is created (8 dimensions for discrete states and 1 dimension for 4 possible actions).</p>
<h3 class="heading" id="sample-code">
  Sample Code
  <a class="anchor" href="#sample-code">#</a>
</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> gymnasium <span style="color:#66d9ef">as</span> gym
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">QLearningAgent</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, state_bins<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.99</span>, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Create LunarLander environment with graphical rendering</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>env <span style="color:#f92672">=</span> gym<span style="color:#f92672">.</span>make(<span style="color:#e6db74">&#34;LunarLander-v3&#34;</span>, render_mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;human&#34;</span>)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Number of bins for each state variable</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>state_bins <span style="color:#f92672">=</span> state_bins
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Discrete state space: 8 variables, each divided into state_bins</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>state_space <span style="color:#f92672">=</span> [state_bins] <span style="color:#f92672">*</span> <span style="color:#ae81ff">8</span>  <span style="color:#75715e"># Example: [10, 10, 10, 10, 10, 10, 10, 10]</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Number of possible actions in the environment (4 actions)</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>action_space <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Create Q-table as a zero array with dimensions (10,10,10,10,10,10,10,10,4)</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>q_table <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(self<span style="color:#f92672">.</span>state_space <span style="color:#f92672">+</span> [self<span style="color:#f92672">.</span>action_space])
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;Q-table dimensions:&#34;</span>, self<span style="color:#f92672">.</span>q_table<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Learning parameter settings</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>lr <span style="color:#f92672">=</span> learning_rate    <span style="color:#75715e"># Learning rate</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>gamma <span style="color:#f92672">=</span> gamma         <span style="color:#75715e"># Discount factor</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>epsilon <span style="color:#f92672">=</span> epsilon     <span style="color:#75715e"># Initial epsilon value for Îµ-greedy policy</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>epsilon_decay <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.995</span> <span style="color:#75715e"># Epsilon decay over time</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>epsilon_min <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Define ranges for state variables for discretization</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>state_bounds <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>            (<span style="color:#f92672">-</span><span style="color:#ae81ff">1.5</span>, <span style="color:#ae81ff">1.5</span>),     <span style="color:#75715e"># x position</span>
</span></span><span style="display:flex;"><span>            (<span style="color:#f92672">-</span><span style="color:#ae81ff">1.5</span>, <span style="color:#ae81ff">1.5</span>),     <span style="color:#75715e"># y position</span>
</span></span><span style="display:flex;"><span>            (<span style="color:#f92672">-</span><span style="color:#ae81ff">5.0</span>, <span style="color:#ae81ff">5.0</span>),     <span style="color:#75715e"># x velocity</span>
</span></span><span style="display:flex;"><span>            (<span style="color:#f92672">-</span><span style="color:#ae81ff">5.0</span>, <span style="color:#ae81ff">5.0</span>),     <span style="color:#75715e"># y velocity</span>
</span></span><span style="display:flex;"><span>            (<span style="color:#f92672">-</span><span style="color:#ae81ff">3.14</span>, <span style="color:#ae81ff">3.14</span>),   <span style="color:#75715e"># angle</span>
</span></span><span style="display:flex;"><span>            (<span style="color:#f92672">-</span><span style="color:#ae81ff">5.0</span>, <span style="color:#ae81ff">5.0</span>),     <span style="color:#75715e"># angular velocity</span>
</span></span><span style="display:flex;"><span>            (<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>),          <span style="color:#75715e"># left leg contact</span>
</span></span><span style="display:flex;"><span>            (<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)           <span style="color:#75715e"># right leg contact</span>
</span></span><span style="display:flex;"><span>        ]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">discretize_state</span>(self, state):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Convert continuous state to a tuple of discrete indices.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        For example, if state = [0.0, 0.5, ...], it will be converted to (5, 7, ...).
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        discrete_state <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i, s <span style="color:#f92672">in</span> enumerate(state):
</span></span><span style="display:flex;"><span>            low, high <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>state_bounds[i]
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Scale the value to the range 0 to 1</span>
</span></span><span style="display:flex;"><span>            scaled <span style="color:#f92672">=</span> (s <span style="color:#f92672">-</span> low) <span style="color:#f92672">/</span> (high <span style="color:#f92672">-</span> low)
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Convert the value to a bin index (from 0 to state_bins - 1)</span>
</span></span><span style="display:flex;"><span>            discrete <span style="color:#f92672">=</span> int(scaled <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>state_bins)
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Ensure the index is within the valid range</span>
</span></span><span style="display:flex;"><span>            discrete <span style="color:#f92672">=</span> max(<span style="color:#ae81ff">0</span>, min(self<span style="color:#f92672">.</span>state_bins <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>, discrete))
</span></span><span style="display:flex;"><span>            discrete_state<span style="color:#f92672">.</span>append(discrete)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> tuple(discrete_state)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">choose_action</span>(self, state):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Choose an action using the Îµ-greedy policy:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        - With probability Îµ, a random action is chosen.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        - Otherwise, the action with the highest Q-value in that state is chosen.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>random() <span style="color:#f92672">&lt;</span> self<span style="color:#f92672">.</span>epsilon:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(self<span style="color:#f92672">.</span>action_space)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>argmax(self<span style="color:#f92672">.</span>q_table[state])
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">learn</span>(self, state, action, reward, next_state):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Update the Q-value in the Q-table using the Q-Learning formula.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        old_value <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>q_table[state <span style="color:#f92672">+</span> (action,)]
</span></span><span style="display:flex;"><span>        next_max <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>max(self<span style="color:#f92672">.</span>q_table[next_state])
</span></span><span style="display:flex;"><span>        new_value <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>lr) <span style="color:#f92672">*</span> old_value <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>lr <span style="color:#f92672">*</span> (reward <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>gamma <span style="color:#f92672">*</span> next_max)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>q_table[state <span style="color:#f92672">+</span> (action,)] <span style="color:#f92672">=</span> new_value
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train</span>(self, episodes<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Train the agent for a specified number of episodes.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        In each episode:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        - The environment is reset, and the initial state is obtained.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        - Steps are taken in the environment until the episode ends (either done or truncated).
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        - At each step:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            - The continuous state is converted to a discrete state.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            - An action is chosen and executed.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            - The Q-table is updated.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        scores <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> episode <span style="color:#f92672">in</span> range(episodes):
</span></span><span style="display:flex;"><span>            state, _ <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>env<span style="color:#f92672">.</span>reset()
</span></span><span style="display:flex;"><span>            total_reward <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>            done <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>            truncated <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">--- Starting Episode </span><span style="color:#e6db74">{</span>episode<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> ---&#34;</span>)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> (done <span style="color:#f92672">or</span> truncated):
</span></span><span style="display:flex;"><span>                <span style="color:#75715e"># Convert continuous state to discrete state, e.g., (5, 5, 3, 2, 7, 4, 0, 1)</span>
</span></span><span style="display:flex;"><span>                discrete_state <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>discretize_state(state)
</span></span><span style="display:flex;"><span>                <span style="color:#75715e"># Choose action using Îµ-greedy policy</span>
</span></span><span style="display:flex;"><span>                action <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>choose_action(discrete_state)
</span></span><span style="display:flex;"><span>                <span style="color:#75715e"># Execute action in the environment</span>
</span></span><span style="display:flex;"><span>                next_state, reward, done, truncated, _ <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>env<span style="color:#f92672">.</span>step(action)
</span></span><span style="display:flex;"><span>                discrete_next_state <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>discretize_state(next_state)
</span></span><span style="display:flex;"><span>                <span style="color:#75715e"># Update Q-table</span>
</span></span><span style="display:flex;"><span>                self<span style="color:#f92672">.</span>learn(discrete_state, action, reward, discrete_next_state)
</span></span><span style="display:flex;"><span>                state <span style="color:#f92672">=</span> next_state
</span></span><span style="display:flex;"><span>                total_reward <span style="color:#f92672">+=</span> reward
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Decrease epsilon to reduce exploration after each episode</span>
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>epsilon <span style="color:#f92672">=</span> max(self<span style="color:#f92672">.</span>epsilon_min, self<span style="color:#f92672">.</span>epsilon <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>epsilon_decay)
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Episode </span><span style="color:#e6db74">{</span>episode<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span><span style="color:#e6db74">}</span><span style="color:#e6db74">/</span><span style="color:#e6db74">{</span>episodes<span style="color:#e6db74">}</span><span style="color:#e6db74"> completed. Score: </span><span style="color:#e6db74">{</span>total_reward<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">, Îµ: </span><span style="color:#e6db74">{</span>self<span style="color:#f92672">.</span>epsilon<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>            scores<span style="color:#f92672">.</span>append(total_reward)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>env<span style="color:#f92672">.</span>close()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> scores
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Run the training example</span>
</span></span><span style="display:flex;"><span>agent <span style="color:#f92672">=</span> QLearningAgent()
</span></span><span style="display:flex;"><span>scores <span style="color:#f92672">=</span> agent<span style="color:#f92672">.</span>train(episodes<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)
</span></span></code></pre></div><p>Code Explanation:
Creating the Q-table:
Using an array with dimensions [10, 10, 10, 10, 10, 10, 10, 10, 4], for each combination of 8 discrete variables (each with 10 possible values), 4 values for different actions are stored.</p>
<p>Discretization:
The discretize_state function converts continuous state values (e.g., speed, angle, etc.) into discrete indices for use in the Q-table.</p>
<p>Action Selection:
Using the Îµ-greedy policy, for each discrete state, an appropriate action is chosen and executed, leading to a reward and the observation of the next state.</p>
<p>Q-table Update:
The Q-value update formula is designed so that the agent uses past experiences to improve future performance.</p>
<p>Training Over Multiple Episodes:
The model is trained over several episodes, and after each episode, Îµ is reduced to gradually shift the agent from exploration to exploitation.</p>
<ol start="5">
<li>Conclusion
In this article, we explored the reinforcement learning process, particularly the Q-Learning algorithm. We highlighted important points such as the need for discretization of continuous state spaces and the use of Q-tables to store Q-values. We also provided a practical example, detailing the steps to create and train an agent.</li>
</ol>
<p>We hope this article has clarified the main concepts of reinforcement learning and the challenges of working with continuous state spaces. In the future, you can deepen your understanding of reinforcement learning algorithms by studying and testing more examples.</p>

    </article>
  </main>
</div>

<style>
.single-container {
  max-width: 800px;
  margin: 0 auto;
  padding: 2rem;
  font-family: monospace;
  line-height: 1.6;
}

.site-title {
  font-size: 2.5rem;
  font-weight: normal;
  margin-bottom: 1.5rem;
  font-family: 'Times New Roman', serif;
}

.content {
  margin-top: 2rem;
}

.content-title {
  font-size: 2rem;
  font-weight: normal;
  margin-bottom: 2rem;
}

 
@media (max-width: 768px) {
  .site-title {
    font-size: 2rem;
  }
  
  .content-title {
    font-size: 1.75rem;
  }
}
</style>

    </main>
  </div>

  <footer>
    

    
    <p>Powered by
        <a href="https://gohugo.io/">Hugo</a>
        and
        <a href="https://github.com/tomfran/typo">tomfran/typo</a>
    </p>
    
    
    


  </footer>

  

</body>

<script src="/js/theme-switch.js"></script>
<script defer src="/js/copy-code.js"></script>
</html>
